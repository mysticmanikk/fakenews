{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39a4b77c-c5dd-4821-81e0-645bfe52d1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id        label                                          statement  \\\n",
      "0   2635.json        false  Says the Annies List political group supports ...   \n",
      "1  10540.json    half-true  When did the decline of coal start? It started...   \n",
      "2    324.json  mostly-true  Hillary Clinton agrees with John McCain \"by vo...   \n",
      "3   1123.json        false  Health care reform legislation is likely to ma...   \n",
      "4   9028.json    half-true  The economic turnaround started at the end of ...   \n",
      "\n",
      "                              subject         speaker             job_title  \\\n",
      "0                            abortion    dwayne-bohac  State representative   \n",
      "1  energy,history,job-accomplishments  scott-surovell        State delegate   \n",
      "2                      foreign-policy    barack-obama             President   \n",
      "3                         health-care    blog-posting                   NaN   \n",
      "4                        economy,jobs   charlie-crist                   NaN   \n",
      "\n",
      "  state_info party_affiliation  barely_true_counts  false_counts  \\\n",
      "0      Texas        republican                 0.0           1.0   \n",
      "1   Virginia          democrat                 0.0           0.0   \n",
      "2   Illinois          democrat                70.0          71.0   \n",
      "3        NaN              none                 7.0          19.0   \n",
      "4    Florida          democrat                15.0           9.0   \n",
      "\n",
      "   half_true_counts  mostly_true_counts  pants_on_fire_counts  \\\n",
      "0               0.0                 0.0                   0.0   \n",
      "1               1.0                 1.0                   0.0   \n",
      "2             160.0               163.0                   9.0   \n",
      "3               3.0                 5.0                  44.0   \n",
      "4              20.0                19.0                   2.0   \n",
      "\n",
      "               context  \n",
      "0             a mailer  \n",
      "1      a floor speech.  \n",
      "2               Denver  \n",
      "3       a news release  \n",
      "4  an interview on CNN  \n",
      "label\n",
      "half-true      2627\n",
      "false          2507\n",
      "mostly-true    2454\n",
      "barely-true    2103\n",
      "true           2053\n",
      "pants-fire     1047\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kmani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kmani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "InvalidIndexError",
     "evalue": "Reindexing only valid with uniquely valued Index objects",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 98\u001b[0m\n\u001b[0;32m     95\u001b[0m     feature_list\u001b[38;5;241m.\u001b[39mappend(extract_features(text))\n\u001b[0;32m     97\u001b[0m features_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(feature_list)\n\u001b[1;32m---> 98\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df, features_df], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Metadata features\u001b[39;00m\n\u001b[0;32m    101\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeaker_freq\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeaker\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeaker\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:395\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    383\u001b[0m     objs,\n\u001b[0;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    393\u001b[0m )\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:680\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m         obj_labels \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39maxes[\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m ax]\n\u001b[0;32m    679\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m new_labels\u001b[38;5;241m.\u001b[39mequals(obj_labels):\n\u001b[1;32m--> 680\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    682\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[0;32m    684\u001b[0m new_data \u001b[38;5;241m=\u001b[39m concatenate_managers(\n\u001b[0;32m    685\u001b[0m     mgrs_indexers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_axes, concat_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbm_axis, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m    686\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3885\u001b[0m, in \u001b[0;36mIndex.get_indexer\u001b[1;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[0;32m   3882\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_method(method, limit, tolerance)\n\u001b[0;32m   3884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_as_unique:\n\u001b[1;32m-> 3885\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_requires_unique_msg)\n\u001b[0;32m   3887\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   3888\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp)\n",
      "\u001b[1;31mInvalidIndexError\u001b[0m: Reindexing only valid with uniquely valued Index objects"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load LIAR dataset\n",
    "train_df = pd.read_csv('train.tsv', sep='\\t', header=None)\n",
    "valid_df = pd.read_csv('valid.tsv', sep='\\t', header=None)\n",
    "test_df = pd.read_csv('test.tsv', sep='\\t', header=None)\n",
    "\n",
    "# Column names from dataset documentation\n",
    "columns = ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', \n",
    "           'state_info', 'party_affiliation', 'barely_true_counts', \n",
    "           'false_counts', 'half_true_counts', 'mostly_true_counts', \n",
    "           'pants_on_fire_counts', 'context']\n",
    "\n",
    "train_df.columns = columns\n",
    "valid_df.columns = columns\n",
    "test_df.columns = columns\n",
    "\n",
    "# Combine all data for preprocessing\n",
    "df = pd.concat([train_df, valid_df, test_df])\n",
    "\n",
    "# Explore the data\n",
    "print(df.head())\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textstat import flesch_reading_ease, smog_index, flesch_kincaid_grade\n",
    "from textblob import TextBlob\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Binary classification (simplify multi-class labels)\n",
    "def simplify_label(label):\n",
    "    if label in ['true', 'mostly-true', 'half-true']:\n",
    "        return 'real'\n",
    "    else:\n",
    "        return 'fake'\n",
    "\n",
    "df['binary_label'] = df['label'].apply(simplify_label)\n",
    "\n",
    "# Text preprocessing\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special chars\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    # Remove single chars\n",
    "    text = re.sub(r'\\s+[a-z]\\s+', ' ', text, flags=re.I)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Lemmatization\n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['cleaned_text'] = df['statement'].apply(preprocess_text)\n",
    "\n",
    "# Feature extraction\n",
    "def extract_features(text):\n",
    "    features = {}\n",
    "    \n",
    "    # Readability features\n",
    "    features['flesch_reading_ease'] = flesch_reading_ease(text)\n",
    "    features['smog_index'] = smog_index(text)\n",
    "    features['flesch_kincaid_grade'] = flesch_kincaid_grade(text)\n",
    "    \n",
    "    # Text statistics\n",
    "    features['char_count'] = len(text)\n",
    "    features['word_count'] = len(text.split())\n",
    "    features['avg_word_length'] = features['char_count'] / max(1, features['word_count'])\n",
    "    features['sentence_count'] = len(re.split(r'[.!?]', text))\n",
    "    \n",
    "    # Sentiment features\n",
    "    blob = TextBlob(text)\n",
    "    features['polarity'] = blob.sentiment.polarity\n",
    "    features['subjectivity'] = blob.sentiment.subjectivity\n",
    "    \n",
    "    # Style features\n",
    "    features['exclamation_count'] = text.count('!')\n",
    "    features['question_count'] = text.count('?')\n",
    "    features['uppercase_count'] = sum(1 for c in text if c.isupper())\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Apply feature extraction\n",
    "feature_list = []\n",
    "for text in df['statement']:\n",
    "    feature_list.append(extract_features(text))\n",
    "\n",
    "features_df = pd.DataFrame(feature_list)\n",
    "df = pd.concat([df, features_df], axis=1)\n",
    "\n",
    "# Metadata features\n",
    "df['speaker_freq'] = df.groupby('speaker')['speaker'].transform('count')\n",
    "df['party_affiliation'] = df['party_affiliation'].fillna('unknown')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Select features\n",
    "text_features = 'cleaned_text'\n",
    "meta_features = ['flesch_reading_ease', 'smog_index', 'flesch_kincaid_grade',\n",
    "                'char_count', 'word_count', 'avg_word_length', 'sentence_count',\n",
    "                'polarity', 'subjectivity', 'exclamation_count', 'question_count',\n",
    "                'uppercase_count', 'speaker_freq']\n",
    "target = 'binary_label'\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[meta_features + [text_features]], \n",
    "    df[target], \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=df[target]\n",
    ")\n",
    "\n",
    "# Preprocessing pipeline\n",
    "text_transformer = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000))\n",
    "])\n",
    "\n",
    "meta_transformer = Pipeline([\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('text', text_transformer, text_features),\n",
    "    ('meta', meta_transformer, meta_features)\n",
    "])\n",
    "\n",
    "# Models to try\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    'XGBoost': XGBClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    results[name] = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_proba),\n",
    "        'report': classification_report(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"=\"*80)\n",
    "\n",
    "# Save the best model\n",
    "best_model_name = max(results, key=lambda x: results[x]['roc_auc'])\n",
    "best_model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', models[best_model_name])\n",
    "])\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "import joblib\n",
    "joblib.dump(best_model, 'fake_news_detector.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9df1302-47cd-4dfc-8a9f-520b3be1d424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\kmani\\anaconda3\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\kmani\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\kmani\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\kmani\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kmani\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\kmani\\anaconda3\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n",
      "Downloading textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "   ---------------------------------------- 0.0/624.3 kB ? eta -:--:--\n",
      "   --------------------------------- ------ 524.3/624.3 kB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 624.3/624.3 kB 3.9 MB/s eta 0:00:00\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.19.0\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4da305-6767-4772-bb5b-dd68036a6534",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
